# -*- coding: utf-8 -*-
"""ALDA_P8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MLrt6cnhuqwl1h3oBWJIIb0H0RtnRTbi

<h1><center> Chicago Car Crash Injury Classification </center></h1>
"""

import time, os
from datetime import datetime
import math
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()
import plotly.express as px
from plotly.subplots import make_subplots
import plotly.graph_objects as go
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, KFold,  GridSearchCV
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif
import imblearn
from imblearn.under_sampling import RandomUnderSampler, NearMiss
from imblearn.over_sampling import RandomOverSampler, SMOTE
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from xgboost import XGBClassifier
from sklearn import metrics
from sklearn.metrics import f1_score

!pip install --quiet shap==0.39.0
import shap

# Warning suppression
import warnings
warnings.filterwarnings('ignore')

"""## Data Import"""

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('drive/MyDrive/ALDAP/Traffic_Crashes_-_Crashes.csv')

data.head()

"""# Data Exploration"""

print("Shape of the data: {}".format(data.shape))

data.columns

print("Total number of columns: {}".format(len(data.columns)))

data.dtypes

print("Number of integer columns: {}".format(data.dtypes.tolist().count(data.dtypes[4])))
print("Number of object columns: {}".format(data.dtypes.tolist().count(data.dtypes[0])))

duplicate = data[data.duplicated() == True]
print("Number of duplicate rows: {}".format(len(duplicate)))

data.isna().sum()

print("Total number of missing values: {}".format(data.isna().sum().sum()))
print("Number of columns with missing values: {}".format(len(data.isnull().sum().tolist())-data.isnull().sum().tolist().count(0)))
print("Number of columns without missing values: {}".format(data.isnull().sum().tolist().count(0)))

# Summary of the data
data.info()

data.describe(include = "all").head(4).T

df = data

def create_bar_plot(format, title, col1, col2, xlab, ylab):
  fig, ax = plt.subplots()
  if (format == "horizontal"):
    bars = ax.barh(col1, col2)
    ax.bar_label(bars)
  else:
    bars = ax.bar(col1, col2)
    plt.xticks(range(len(col1)), col1)
  plt.title(title)
  plt.xlabel(xlab)
  plt.ylabel(ylab)
  plt.show()

### Exploratory Data Analysis

## Looking at our possible predictor variables

# Most Severe Injury
m_severe_v_counts = df.MOST_SEVERE_INJURY.value_counts()
most_severe_injury_counts = pd.DataFrame({"Injury Severity" : m_severe_v_counts.index, "Count" : m_severe_v_counts.values})

# Displaying the counts for each level of MOST_SEVERE_INJURY
print(most_severe_injury_counts)
print()

# Creating a horizontal bar plot to display this same information graphically
create_bar_plot("horizontal", "Injury Severities for Reported Car Crashes in Chicago",
                most_severe_injury_counts["Injury Severity"], most_severe_injury_counts["Count"], "Count",
                "Injury Severity")

# Dropping rows na values
m_severe_sub_df = df[df["MOST_SEVERE_INJURY"].isna() == False]
m_severe_unique = m_severe_sub_df.MOST_SEVERE_INJURY.unique()

inj = m_severe_sub_df[m_severe_sub_df["POSTED_SPEED_LIMIT"] >= 5]
inj = inj[inj["POSTED_SPEED_LIMIT"] <= 70]
inj = inj[inj["POSTED_SPEED_LIMIT"] % 5 == 0]

comp_speed_freqs = inj.POSTED_SPEED_LIMIT.value_counts()
comp_injury_speed_lim_freq = pd.DataFrame({"Speed Limit" : comp_speed_freqs.index, "Count" : comp_speed_freqs.values})

comp_injury_speed_lim_freq = comp_injury_speed_lim_freq.sort_values(by = "Speed Limit")

comp_injury_speed_lim_freq["Percentage"] = comp_injury_speed_lim_freq["Count"] / len(m_severe_sub_df)
print("One-Way Frequency Table for all Injury Levels")
print(comp_injury_speed_lim_freq)
print()

create_bar_plot("horizontal", "One-Way Frequency Table of Speed Limits for all Injury Levels",
                comp_injury_speed_lim_freq["Speed Limit"], comp_injury_speed_lim_freq["Count"], "Count",
                "Speed Limit")

# Loops through the severity levels
for severity_level in m_severe_unique:
  # Subsets according the current injury level
  injury_severity = m_severe_sub_df[m_severe_sub_df["MOST_SEVERE_INJURY"] == severity_level]

  # Subsets the speed limits to only include valid speed limits
  injuries = injury_severity[injury_severity["POSTED_SPEED_LIMIT"] >= 5]
  injuries = injuries[injuries["POSTED_SPEED_LIMIT"] <= 70]
  injuries = injuries[injuries["POSTED_SPEED_LIMIT"] % 5 == 0]

  # Finds the frequencies of the speed limits and severity levels and then converts the series into a data frame
  speed_freqs = injuries.POSTED_SPEED_LIMIT.value_counts()
  injury_speed_lim_freq = pd.DataFrame({"Speed Limit" : speed_freqs.index, "Count" : speed_freqs.values})

  # Sorts the data frame by speed limit so that the display is cleaner when plotting the data
  injury_speed_lim_freq = injury_speed_lim_freq.sort_values(by = "Speed Limit")

  # Finishes the set up for the one-way frequencies
  injury_speed_lim_freq["Percentage"] = injury_speed_lim_freq["Count"] / len(injury_severity)
  print("One-Way Frequency Table for the " + severity_level + " Injury Level")
  print(injury_speed_lim_freq)
  print()

  # Plots the frequencies
  title = "Speed Limits for the " + severity_level + " Injury Level"
  create_bar_plot("horizontal", title,
                injury_speed_lim_freq["Speed Limit"], injury_speed_lim_freq["Count"], "Count",
                "Speed Limit")

comp_crash_type_freqs = inj.FIRST_CRASH_TYPE.value_counts()
comp_injury_crash_type_freq = pd.DataFrame({"Crash Type" : comp_crash_type_freqs.index,
                                           "Count" : comp_crash_type_freqs.values})

comp_injury_crash_type_freq["Percentage"] = comp_injury_crash_type_freq["Count"] / len(m_severe_sub_df)
print("One-Way Frequency Table for all Injury Levels")
print(comp_injury_crash_type_freq)
print()

title = "Crash Type for all Injury Levels"
create_bar_plot("horizontal", title,
                comp_injury_crash_type_freq["Crash Type"], comp_injury_crash_type_freq["Count"], "Count",
                "Crash Type")

# Loops through the severity levels
for severity_level in m_severe_unique:
  # Subsets according the current injury level
  injury_severity = m_severe_sub_df[m_severe_sub_df["MOST_SEVERE_INJURY"] == severity_level]

  crash_type_freqs = injury_severity.FIRST_CRASH_TYPE.value_counts()
  injury_crash_type_freq = pd.DataFrame({"Crash Type" : crash_type_freqs.index, "Count" : crash_type_freqs.values})

  injury_crash_type_freq["Percentage"] = injury_crash_type_freq["Count"] / len(injury_severity)
  print("One-Way Frequency Table for the " + severity_level + " Injury Level")
  print(injury_crash_type_freq)
  print()

  title = "Crash Types for the " + severity_level + " Injury Level"

  create_bar_plot("horizontal", title,
                  injury_crash_type_freq["Crash Type"], injury_crash_type_freq["Count"], "Count",
                  "Crash Type")

three_way_sv_sl_ct_table_frame = m_severe_sub_df[["MOST_SEVERE_INJURY", "POSTED_SPEED_LIMIT", "FIRST_CRASH_TYPE"]]
three_way_sv_sl_ct_table_frame = three_way_sv_sl_ct_table_frame[three_way_sv_sl_ct_table_frame["POSTED_SPEED_LIMIT"] >= 5]
three_way_sv_sl_ct_table_frame = three_way_sv_sl_ct_table_frame[three_way_sv_sl_ct_table_frame["POSTED_SPEED_LIMIT"] <= 70]
three_way_sv_sl_ct_table_frame = three_way_sv_sl_ct_table_frame[three_way_sv_sl_ct_table_frame["POSTED_SPEED_LIMIT"] % 5 == 0]

severity_speed_limit_crash_type_crosstab = pd.crosstab(index = [three_way_sv_sl_ct_table_frame['MOST_SEVERE_INJURY'],
                                                                three_way_sv_sl_ct_table_frame['POSTED_SPEED_LIMIT']],
                                                       columns = three_way_sv_sl_ct_table_frame["FIRST_CRASH_TYPE"])
severity_speed_limit_crash_type_crosstab

comp_hour_freqs = inj.CRASH_HOUR.value_counts()
comp_injury_hour_freq = pd.DataFrame({"Hour" : comp_hour_freqs.index,
                                           "Count" : comp_hour_freqs.values})

comp_injury_hour_freq = comp_injury_hour_freq.sort_values(by = "Hour")

comp_injury_hour_freq["Percentage"] = comp_injury_hour_freq["Count"] / len(m_severe_sub_df)
print("One-Way Frequency Table for all Injury Levels")
print(comp_injury_hour_freq)
print()

title = "Accident Hour for all Injury Levels"
create_bar_plot("vertical", title,
                comp_injury_hour_freq["Hour"], comp_injury_hour_freq["Count"], "Hour",
                "Count")

for severity_level in m_severe_unique:

  injury_severity = m_severe_sub_df[m_severe_sub_df["MOST_SEVERE_INJURY"] == severity_level]

  hour_freqs = injury_severity.CRASH_HOUR.value_counts()
  injury_hour_freq = pd.DataFrame({"Hour" : hour_freqs.index, "Count" : hour_freqs.values})

  injury_hour_freq = injury_hour_freq.sort_values(by = "Hour")

  injury_hour_freq["Percentage"] = injury_hour_freq["Count"] / len(injury_severity)
  print("One-Way Frequency Table for the " + severity_level + " Injury Level")
  print(injury_hour_freq)
  print()

  title = "Hour of Accidents for the " + severity_level + " Injury Level"
  create_bar_plot("vertical", title,
                  injury_hour_freq["Hour"], injury_hour_freq["Count"], "Hour",
                  "Count")

for severity_level in m_severe_unique:

  injury_severity = m_severe_sub_df[m_severe_sub_df["MOST_SEVERE_INJURY"] == severity_level]

  weather_freqs = injury_severity.WEATHER_CONDITION.value_counts()
  injury_weather_freq = pd.DataFrame({"Weather Condition" : weather_freqs.index, "Count" : weather_freqs.values})

  injury_weather_freq["Percentage"] = injury_weather_freq["Count"] / len(injury_severity)
  print("One-Way Frequency Table for the " + severity_level + " Injury Level")
  print(injury_weather_freq)
  print()

  title = "Weather of Accidents for the " + severity_level + " Injury Level"
  create_bar_plot("horizontal", title,
                  injury_weather_freq["Weather Condition"], injury_weather_freq["Count"], "Count",
                  "Weather Condition")

"""# Data Cleaning and Preprocessing"""

def combine_similar(data):
    data = data.replace('Unknown', 'unknown')
    data = data.replace('Other', 'other')
    data = data.replace('Unknown or other', 'other')
    return data

data = combine_similar(data)

data.isna().sum()[data.isna().sum() != 0].sort_values(ascending = False)

data.T.isna().sum()[data.T.isna().sum() != 0].sort_values(ascending = False)

def more_missing(data):
    data = data.replace('unknown', np.nan)
    data = data.replace('other', np.nan)
    data = data.replace('na', np.nan)
    return data

data = more_missing(data)

def prop_imputer(data):
    data_prop = data.copy(deep = True)
    missing_cols = data_prop.isna().sum()[data_prop.isna().sum() != 0].index.tolist()
    for col in missing_cols:
        values_col = data_prop[col].value_counts(normalize = True).index.tolist()
        probabilities_col = data_prop[col].value_counts(normalize = True).values.tolist()
        data_prop[col] = data_prop[col].fillna(pd.Series(np.random.choice(values_col, p = probabilities_col, size = len(data))))
    return data_prop

data_imp = prop_imputer(data)

data = data_imp
data.head()

data.isna().sum()

# Removing invalid Speed Limit values
data = data[data["POSTED_SPEED_LIMIT"] >= 5]
data = data[data["POSTED_SPEED_LIMIT"] <= 70]
data = data[data["POSTED_SPEED_LIMIT"] % 5 == 0]

# Dropping columns that we probably don't need
cols2drop = ['INJURIES_TOTAL', 'CRASH_RECORD_ID', 'RD_NO', 'CRASH_DATE_EST_I', 'REPORT_TYPE', 'BEAT_OF_OCCURRENCE',
             'PHOTOS_TAKEN_I', 'STATEMENTS_TAKEN_I', 'DOORING_I', 'WORK_ZONE_I', 'WORK_ZONE_TYPE', 'WORKERS_PRESENT_I',
            'INJURIES_FATAL', 'INJURIES_INCAPACITATING', 'INJURIES_NON_INCAPACITATING',
             'INJURIES_REPORTED_NOT_EVIDENT', 'INJURIES_NO_INDICATION', 'INJURIES_UNKNOWN', 'LOCATION']
data = data.drop(cols2drop, axis=1)

# Reducing out classification to those of injuries
classification_keep = ["NONINCAPACITATING INJURY", "INCAPACITATING INJURY", "FATAL"]

# Subseting using the above list
data = data[data["MOST_SEVERE_INJURY"].isin(classification_keep)]

numerical_cols = ['POSTED_SPEED_LIMIT', 'NUM_UNITS', 'LATITUDE', 'LONGITUDE']
ordinal_cols = ['CRASH_HOUR', 'CRASH_DAY_OF_WEEK', 'CRASH_MONTH']
nominal_cols = [x for x in data.columns if x not in numerical_cols]
nominal_cols = [y for y in nominal_cols if y not in ordinal_cols]

nominal_cols

# Dict for saving model and encoder
saved_dictionary = {}

def label_encoder(data, cols):
    data_le = data.copy(deep = True)
    for col in cols:
        print(f'Encoding {col}')
        le = LabelEncoder()
        data_le[col] = le.fit_transform(data_le[col])
        saved_dictionary[f'le_{col}'] = le
    return data_le

Hour_dictionary = {x: x for x in range(24)}
Day_dictionary = {x: x for x in range(1, 8)}
Month_dictionary = {x: x for x in range(1, 13)}
Severity_dictionary = {"NONINCAPACITATING INJURY" : 0, "INCAPACITATING INJURY" : 1, "FATAL" : 2}

manual_encoder_dictionary = {'CRASH_HOUR': Hour_dictionary,
                       'CRASH_DAY_OF_WEEK' : Day_dictionary,
                       'CRASH_MONTH' : Month_dictionary,
                       'MOST_SEVERE_INJURY' : Severity_dictionary
                      }

def manual_encoder(data, ordinal_cols, manual_encoder_dictionary):
    data_me = data.copy(deep = True)
    for feature in data.columns:
        if feature in ordinal_cols:
            if feature != 'Accident_severity':
                data_me[feature] = data_me[feature].map(manual_encoder_dictionary[feature])
    return data_me

data = manual_encoder(data, ordinal_cols, manual_encoder_dictionary)
data = label_encoder(data, nominal_cols)

data.head()

def heatmap(data):
    plt.figure(figsize = (36, 27))
    sns.heatmap(data.corr(), annot = True, cmap = plt.cm.CMRmap_r)

data_heat = data.drop('MOST_SEVERE_INJURY', axis = 1)
heatmap(data_heat)

drop_cols = ['DATE_POLICE_NOTIFIED']
data = data.drop(drop_cols, axis = 1)

"""## Experiments"""

def predictor_target_split(data, target):
  y = data[target]
  X = data.drop(target, axis = 1)
  return X, y

X, y = predictor_target_split(data, 'MOST_SEVERE_INJURY')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

future_comp = y_train

def smote(X_train, y_train):
  smote = SMOTE(random_state = 42, n_jobs = -1)
  X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
  return X_train_smote, y_train_smote

def resampler(X_train, y_train, method = smote, countplot = True):
  if method not in [smote]:
    print("Method invalid")
  else:
    X_train_new, y_train_new = method(X_train, y_train)
    return X_train_new, y_train_new

X_train, y_train = resampler(X_train, y_train, method = smote, countplot = True)

y_train.value_counts()

future_comp.value_counts()

def spread_pos(data):
  data_positive = data.copy(deep = True)
  for feature in data_positive.columns:
    if np.any(data_positive[feature] < 0) == True:
      data_positive[feature] = data_positive[feature] - data_positive[feature].min()
  return data_positive

def spread_pos_series(data):
  data_positive = data.copy(deep = True)
  if np.any(data_positive < 0) == True:
    data_positive = data_positive - data_positive.min()
  return data_positive

def feature_selection_chi2(X_train, y_train, X_test, k = 'all'):
  if np.any(X_train < 0) == True:
    X_train = spread_pos(X_train)
  if np.any(y_train < 0) == True:
    y_train = spread_pos_series(y_train)
  if np.any(X_test < 0) == True:
    X_test = spread_pos(X_test)

  fs = SelectKBest(score_func = chi2, k = k)
  fs.fit(X_train, y_train)

  cols = fs.get_support(indices = True)

  X_train_fs = X_train.iloc[:, cols]
  X_test_fs = X_test.iloc[:, cols]

  return X_train_fs, X_test_fs, fs

def feature_selection_mutual_information(X_train, y_train, X_test, k = 'all'):
  fs = SelectKBest(score_func = mutual_info_classif, k = k)
  fs.fit(X_train, y_train)
  cols = fs.get_support(indices = True)
  X_train_fs = X_train.iloc[:, cols]
  X_test_fs = X_test.iloc[:, cols]
  return X_train_fs, X_test_fs, fs

X_train, X_test, fs = feature_selection_chi2(X_train, y_train, X_test, k = 'all')

plt.figure(figsize = (16, 5))
sns.barplot(x = [i for i in range(len(fs.scores_))], y = fs.scores_)
plt.show()

X_train, X_test, fs = feature_selection_chi2(X_train, y_train, X_test, k = 16)

def conf_mat(y_pred, y_test):
    class_names = ['Non-Incap', 'Incap', 'Fatal']
    tick_marks_y = [0.5, 1.5, 2.5]
    tick_marks_x = [0.5, 1.5, 2.5]
    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)
    confusion_matrix_df = pd.DataFrame(confusion_matrix, range(3), range(3))
    plt.figure(figsize = (6, 4.75))
    sns.set(font_scale = 1.4) # label size
    plt.title("Confusion Matrix")
    sns.heatmap(confusion_matrix_df, annot = True, annot_kws = {"size": 16}, fmt = 'd') # font size
    plt.yticks(tick_marks_y, class_names, rotation = 'vertical')
    plt.xticks(tick_marks_x, class_names, rotation = 'horizontal')
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.grid(False)
    plt.show()

f1_dict = {}

dt = DecisionTreeClassifier(random_state = 42)
dt.fit(X_train, y_train)
y_pred = dt.predict(X_test)
score = f1_score(y_test, y_pred, average = 'weighted')
print("Weighted F1-score on the test set: {}".format(score))
f1_dict['Decision Tree'] = score
conf_mat(y_pred, y_test)

rf = RandomForestClassifier(random_state = 42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)
score = f1_score(y_test, y_pred, average = 'weighted')
print("Weighted F1-score on the test set: {}".format(score))
f1_dict['Random Forest'] = score
conf_mat(y_pred, y_test)

xgb = XGBClassifier(random_state = 42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)
score = f1_score(y_test, y_pred, average = 'weighted')
print("weighted F1-score on the test set: {}".format(score))
f1_dict['XGBoost'] = score
conf_mat(y_pred_xgb, y_test)

extree = ExtraTreesClassifier(random_state = 42)
extree.fit(X_train, y_train)
y_pred = extree.predict(X_test)
score = f1_score(y_test, y_pred, average = 'weighted')
print("Weighted F1-score on the test set: {}".format(score))
f1_dict['ExtraTrees'] = score
conf_mat(y_pred, y_test)

f1_df = pd.DataFrame(f1_dict.items(), columns = ['Classifier', 'F1-score'])
f1_df.sort_values(by = ['F1-score'], ascending = False)

# W/ K-Fold cross validation to find optimal parameters for a couple of the algorithms
f1_dict_ht = {}

cv = KFold(n_splits = 5, shuffle = True, random_state = 42).split(X = X_train, y = y_train)
rf = RandomForestClassifier()
params_rf = {'n_estimators': [100, 200],
          'criterion': ['gini', 'entropy'],
          'max_depth': [None],
          'max_features': ['auto', 'sqrt', 'log2'],
          'class_weight': ['balanced', None]
          }
gsearch_rf = GridSearchCV(estimator = rf, param_grid = params_rf, scoring = 'f1_weighted', n_jobs = -1, cv = cv, verbose = 3)
gsearch_rf_fit = gsearch_rf.fit(X = X_train, y = y_train)
print("Best parameters: {}".format(gsearch_rf.best_params_))
print("Best weighted F1-score: {}".format(gsearch_rf.best_score_))
print(" ")

rf_best = gsearch_rf.best_estimator_
rf_best.fit(X_train, y_train)
y_pred_rf = rf_best.predict(X_test)
score = f1_score(y_test, y_pred_rf, average = 'weighted')
print("Weighted F1-score on the test set: {}".format(score))
f1_dict_ht['Random Forest'] = score
conf_mat(y_pred_rf, y_test)

cv = KFold(n_splits = 5, shuffle = True, random_state = 42).split(X = X_train, y = y_train)
xgb = XGBClassifier()
params_xgb = {'min_child_weight': [1, 5, 10],
              'gamma': [0.5, 2, 5],
              'subsample': [0.6, 1.0],
              'colsample_bytree': [0.6],
              'max_depth': [4, 5],
              }
gsearch_xgb = GridSearchCV(estimator = xgb, param_grid = params_xgb, scoring = 'f1_weighted', n_jobs = -1, cv = cv, verbose = 3)
gsearch_xgb_fit = gsearch_xgb.fit(X = X_train, y = y_train)
print("Best parameters: {}".format(gsearch_xgb.best_params_))
print("Best weighted F1-score: {}".format(gsearch_xgb.best_score_))
print(" ")

xgb_best = gsearch_xgb.best_estimator_
xgb_best.fit(X_train, y_train)
y_pred_xgb = xgb_best.predict(X_test)
score = f1_score(y_test, y_pred_xgb, average = 'weighted')
print("Weighted F1-score on the test set: {}".format(score))
f1_dict_ht['ExtraTrees'] = score
conf_mat(y_pred_xgb, y_test)

f1_df_ht = pd.DataFrame(f1_dict_ht.items(), columns = ['Classifier', 'F1-score'])
f1_df_ht.sort_values(by = ['F1-score'], ascending = False)